import { b } from './chunk-YMIPE5DI.js';
import { useEffectAsync, useMemoAsync } from '@chengsokdara/react-hooks-async';
import { useRef, useState, useEffect } from 'react';

var re={apiKey:"",autoStart:!1,autoTranscribe:!0,mode:"transcriptions",nonStop:!1,removeSilence:!1,stopTimeout:5e3,streaming:!1,timeSlice:1e3,onDataAvailable:void 0,onTranscribe:void 0},te={stop:void 0},ne={blob:void 0,text:void 0},ue=L=>{let{apiKey:b$1,autoStart:W,autoTranscribe:U,mode:S,nonStop:O,removeSilence:oe,stopTimeout:F,streaming:x,timeSlice:ae,whisperConfig:u,onDataAvailable:M,onTranscribe:d}={...re,...L};if(!b$1&&!d)throw new Error("apiKey is required if onTranscribe is not provided");let l=useRef([]),s=useRef(),c=useRef(),r=useRef(),o=useRef(),m=useRef(te),[P,T]=useState(!1),[q,k]=useState(!1),w=useRef(!1),[K,R]=useState(!1),[I,g]=useState(ne);useEffect(()=>()=>{l.current&&(l.current=[]),s.current&&(s.current.flush(),s.current=void 0),r.current&&(r.current.destroy(),r.current=void 0),y("stop"),c.current&&(c.current.off("speaking",A),c.current.off("stopped_speaking",v)),o.current&&(o.current.getTracks().forEach(e=>e.stop()),o.current=void 0);},[]),useEffectAsync(async()=>{W&&await B();},[W]);let $=async()=>{await B();},j=async()=>{await N();},z=async()=>{await C();},B=async()=>{try{if(o.current||await G(),o.current){if(!r.current){let{default:{RecordRTCPromisesHandler:t,StereoAudioRecorder:a}}=await import('recordrtc'),n={mimeType:"audio/wav",numberOfAudioChannels:1,recorderType:a,desiredSampRate:16e3,type:"audio",ondataavailable:U&&x?X:void 0};r.current=new t(o.current,n);}if(!s.current){let{Mp3Encoder:t}=await import('lamejs');s.current=new t(1,16e3,96);}let e=await r.current.getState();(e==="inactive"||e==="stopped")&&await r.current.startRecording(),e==="paused"&&await r.current.resumeRecording(),T(!0);}}catch(e){console.error(e);}},G=async()=>{try{if(o.current&&o.current.getTracks().forEach(e=>e.stop()),o.current=await navigator.mediaDevices.getUserMedia({audio:!0}),!c.current){let{default:e}=await import('hark');c.current=e(o.current,{interval:100,play:!1}),c.current.on("speaking",A),c.current.on("stopped_speaking",v);}}catch(e){console.error(e);}},J=e=>{m.current[e]||(m.current[e]=setTimeout(C,F));},A=()=>{console.log("start speaking"),k(!0),w.current=!0,y("stop");},v=()=>{console.log("stop speaking"),k(!1),O&&w.current&&J("stop");},N=async()=>{try{r.current&&(await r.current.getState()==="recording"&&await r.current.pauseRecording(),y("stop"),T(!1));}catch(e){console.error(e);}},C=async()=>{try{if(r.current){let e=await r.current.getState();if((e==="recording"||e==="paused")&&await r.current.stopRecording(),Q(),y("stop"),T(!1),w.current=!1,U)await V();else {let t=await r.current.getBlob();g({blob:t});}await r.current.destroy(),l.current=[],s.current&&(s.current.flush(),s.current=void 0),r.current=void 0;}}catch(e){console.error(e);}},Q=()=>{c.current&&(k(!1),c.current.off("speaking",A),c.current.off("stopped_speaking",v),c.current=void 0),o.current&&(o.current.getTracks().forEach(e=>e.stop()),o.current=void 0);},y=e=>{m.current[e]&&(clearTimeout(m.current[e]),m.current[e]=void 0);},V=async()=>{console.log("transcribing speech");try{if(s.current&&r.current&&await r.current.getState()==="stopped"){R(!0);let t=await r.current.getBlob(),a=await t.arrayBuffer();console.log({wav:a.byteLength});let n=s.current.encodeBuffer(new Int16Array(a)),i=new Blob([n],{type:"audio/mpeg"});if(console.log({mp3blob:i,mp3:n.byteLength}),console.log("HELLO"),typeof d=="function"){let f=await d(t);console.log("onTranscribe",f),g(f);}else {let f=new File([i],"speech.mp3",{type:"audio/mpeg"}),H=await E(f);console.log("onTranscribing",{text:H}),g({blob:t,text:H});}R(!1);}}catch(e){console.info(e),R(!1);}},X=async e=>{console.log("onDataAvailable",e);try{if(x&&r.current){if(M?.(e),s.current){let a=await e.arrayBuffer(),n=s.current.encodeBuffer(new Int16Array(a)),i=new Blob([n],{type:"audio/mpeg"});l.current.push(i);}if(await r.current.getState()==="recording"){let a=new Blob(l.current,{type:"audio/mpeg"});if(typeof d=="function"){let n=await d(a);console.log("onTranscribe",n),n.text&&g(i=>({...i,text:n.text}));}else {let n=new File([a],"speech.mp3",{type:"audio/mpeg"}),i=await E(n);console.log("onTranscribing",{text:i}),i&&g(f=>({...f,text:i}));}}}}catch(t){console.error(t);}},E=useMemoAsync(async e=>{let t=new FormData;t.append("file",e),t.append("model","whisper-1"),S==="transcriptions"&&t.append("language",u?.language??"en"),u?.prompt&&t.append("prompt",u.prompt),u?.response_format&&t.append("response_format",u.response_format),u?.temperature&&t.append("temperature",`${u.temperature}`);let a={};a["Content-Type"]="multipart/form-data",b$1&&(a.Authorization=`Bearer ${b$1}`);let{default:n}=await import('axios');return (await n.post(b+S,t,{headers:a})).data.text},[b$1,S,u]);return {recording:P,speaking:q,spokeAtLeastOnce:w,transcribing:K,transcript:I,pauseRecording:j,startRecording:$,stopRecording:z}};

export { ue as a };
//# sourceMappingURL=out.js.map
//# sourceMappingURL=chunk-7AHKPRFF.js.map